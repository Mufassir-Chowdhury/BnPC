{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iZJ4geNez8v",
        "outputId": "64be9d82-5d51-40f6-b040-725face626f4"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import statistics\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import glob\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOcgYpJIVzXq",
        "outputId": "68e2e484-41ce-4b81-acb7-ddd9d119f72a"
      },
      "source": [
        "! gdown --id 1_C260zgutx-BXmMUkDYfhJTqlDLCsHPI\n",
        "! unzip FinalDataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_C260zgutx-BXmMUkDYfhJTqlDLCsHPI\n",
            "To: /content/FinalDataset.zip\n",
            "100% 163k/163k [00:00<00:00, 80.7MB/s]\n",
            "Archive:  FinalDataset.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n",
            "  inflating: val.csv                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnnI_45qAyaj"
      },
      "source": [
        "def addlabels(x,y):\n",
        "    for i in range(len(x)):\n",
        "        # plt.text(i, y[i]+100, y[i], ha = 'center')\n",
        "        plt.text(y[i]+30, i, y[i], color='black', fontsize=14, ha='left', va='center')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DMHgVKFGe3I3",
        "outputId": "c16ec54a-6acc-44dc-8b60-53d5d4432b6f"
      },
      "source": [
        "from google.colab import files\n",
        "upload=files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e5c8e362-7b61-4666-a031-ecb3b64af311\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e5c8e362-7b61-4666-a031-ecb3b64af311\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pos_whole_dataset.csv to pos_whole_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79VkrIZee5af",
        "outputId": "bdca930e-54ed-4bb1-b175-9459f7c920e9"
      },
      "source": [
        "# dataset = pd.read_excel('pos_whole_dataset.xlsx')\n",
        "# dataset2 = pd.read_excel('pos_whole_dataset.xlsx')\n",
        "# dataset3 = pd.read_excel('Whole_BNPC.xlsx')\n",
        "\n",
        "dataset = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"*.csv\"))))\n",
        "\n",
        "dataset2 = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"*.csv\"))))\n",
        "\n",
        "# dataset = pd.read_csv('train.csv', )\n",
        "\n",
        "# dataset = pd.read_csv('pos_whole_dataset.csv')\n",
        "# dataset2 = pd.read_csv('pos_whole_dataset.csv')\n",
        "# dataset3 = pd.read_csv('val.csv')\n",
        "print(dataset)\n",
        "# print(dataset2)\n",
        "\n",
        "dataset.columns\n",
        "# dataset2.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              sentence1  \\\n",
            "0                      রায়ের পর হাসছিলেন রিফাত ফরাজীরা   \n",
            "1                      রায়ের পর হাসছিলেন রিফাত ফরাজীরা   \n",
            "2                      রায়ের পর হাসছিলেন রিফাত ফরাজীরা   \n",
            "3                      রায়ের পর হাসছিলেন রিফাত ফরাজীরা   \n",
            "4                      রায়ের পর হাসছিলেন রিফাত ফরাজীরা   \n",
            "...                                                 ...   \n",
            "5914     কারা কারা তৃণমূল ছাড়তে পারেন শুভেন্দুর সঙ্গে?   \n",
            "5915  শুভেন্দুকে আটকাতে তৎপর তৃণমূল, দায়িত্বে ২ সাং...   \n",
            "5916  মমতাই ঠিক ছিলেন, একুশের আগে পিকে-অভিষেকের ঘুম ...   \n",
            "5917  মমতাই ঠিক ছিলেন, একুশের আগে পিকে-অভিষেকের ঘুম ...   \n",
            "5918  শুভেন্দুকে লাইনে আনতে বৈঠকে তৃণমূলের শীর্ষ নেতারা   \n",
            "\n",
            "                                              sentence2  label  \n",
            "0               আদালত থেকে আর বাড়ি যাওয়া হলো না মিন্নির      0  \n",
            "1      রিফাত হত্যায় স্ত্রী মিন্নিসহ ৬ জনের ফাঁসির রায়      0  \n",
            "2                মিন্নির প্রতি অবিচার করা হয়েছে : বাবা      0  \n",
            "3            রিফাত হত্যা মামলা : কিছুক্ষণের মধ্যেই রায়      0  \n",
            "4                 আদালতে এসেছেন বিচারক মো. আছাদুজ্জামান      0  \n",
            "...                                                 ...    ...  \n",
            "5914  বরফ গলাতে সক্রিয় তৃণমূল, সাংসদের সঙ্গে বৈঠকে শ...      0  \n",
            "5915  মমতাই ঠিক ছিলেন, একুশের আগে পিকে-অভিষেকের ঘুম ...      0  \n",
            "5916  শুভেন্দুকে লাইনে আনতে বৈঠকে তৃণমূলের শীর্ষ নেতারা      0  \n",
            "5917  বরফ গলাতে সক্রিয় তৃণমূল, সাংসদের সঙ্গে বৈঠকে শ...      0  \n",
            "5918  বরফ গলাতে সক্রিয় তৃণমূল, সাংসদের সঙ্গে বৈঠকে শ...      0  \n",
            "\n",
            "[8787 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence1', 'sentence2', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKJVdvFiTeuH",
        "outputId": "98580140-5002-4c63-a17c-0d65716d6dc9"
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import sys\n",
        "\n",
        "!pip install twokenize\n",
        "from twokenize import tokenize\n",
        "\n",
        "def PINC(source,candidate,N):\n",
        "    tokenize_source=tokenize(source)\n",
        "    tokenize_candidate=tokenize(candidate)\n",
        "    if(len(tokenize_candidate)==0 or len(tokenize_source)==0):\n",
        "        return(float(1))\n",
        "    if(len(tokenize_candidate)<N or len(tokenize_source)<N):\n",
        "        return('illegal')\n",
        "        # print(tokenize_source)\n",
        "        # print(tokenize_candidate)\n",
        "        # print(len(tokenize_source), len(tokenize_candidate), N)\n",
        "        # print(source)\n",
        "        # print(candidate)\n",
        "        # print('Illegal input!')\n",
        "        sys.exit()\n",
        "    sum=0\n",
        "    for i in range(N):\n",
        "        n_gram_s=[]\n",
        "        n_gram_c=[]\n",
        "        for k in range(len(tokenize_source)-i):\n",
        "            n_gram_s.append(tokenize_source[k:k+i+1])\n",
        "        for k in range(len(tokenize_candidate)-i):\n",
        "            n_gram_c.append(tokenize_candidate[k:k+i+1])\n",
        "        # print(n_gram_s)\n",
        "        # print(n_gram_c)\n",
        "        counter=0\n",
        "        for element in n_gram_s:\n",
        "            if(element in n_gram_c):\n",
        "                counter+=1\n",
        "        sum+=1-counter/len(n_gram_c)\n",
        "        #print(counter)\n",
        "        # if(type(sum/N)==str):\n",
        "          # print(sum/N)\n",
        "\n",
        "\n",
        "    return(float(sum/N))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: twokenize in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method7\n",
        "\n",
        "plt.figure(figsize = (7, 7))\n",
        "\n",
        "# plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "total_rows = dataset.shape[0]\n",
        "\n",
        "ref_list = list()\n",
        "cand_list = list()\n",
        "Sentence_length = list()\n",
        "count = 0\n",
        "tot_score2 = list()\n",
        "\n",
        "\n",
        "total_rows2 = dataset2.shape[0]\n",
        "\n",
        "ref_list2 = list()\n",
        "cand_list2 = list()\n",
        "Sentence_length2 = list()\n",
        "count2 = 0\n",
        "tot_score3 = list()\n",
        "\n",
        "\n",
        "for i in range(total_rows):\n",
        "  sentence = dataset[\"sentence1\"][i]\n",
        "  sentence2 = dataset[\"sentence2\"][i]\n",
        "  score=PINC(sentence,sentence2,3)\n",
        "  tot_score2.append(score)\n",
        "  count += 1\n",
        "\n",
        "for i in range(total_rows2):\n",
        "  sentence3 = dataset2[\"sentence1\"][i]\n",
        "  sentence4 = dataset2[\"sentence2\"][i]\n",
        "  score2=PINC(sentence3,sentence4,3)\n",
        "  tot_score3.append(score2)\n",
        "  count2 += 1\n",
        "\n",
        "N = len(tot_score2)\n",
        "\n",
        "tot_score2=[i for i in tot_score2 if i != 'illegal']\n",
        "\n",
        "[float(i) for i in tot_score2 if type(i) == str]\n",
        "\n",
        "\n",
        "b=np.round(tot_score2,1)\n",
        "\n",
        "unique_elements, counts_elements = np.unique(b, return_counts=True)\n",
        "\n",
        "xxi = unique_elements.tolist()\n",
        "ooi = list(map(str, xxi))\n",
        "\n",
        "yyi = counts_elements.tolist()\n",
        "ooi.insert(0,'0.1')\n",
        "ooi.insert(0,'0.0')\n",
        "yyi.insert(0,0)\n",
        "yyi.insert(0,0)\n",
        "\n",
        "\n",
        "N2 = len(tot_score3)\n",
        "\n",
        "tot_score3=[i for i in tot_score3 if i != 'illegal']\n",
        "\n",
        "[float(i) for i in tot_score3 if type(i) == str]\n",
        "\n",
        "\n",
        "b2=np.round(tot_score3,1)\n",
        "\n",
        "unique_elements2, counts_elements2 = np.unique(b2, return_counts=True)\n",
        "\n",
        "xxi2 = unique_elements2.tolist()\n",
        "ooi2 = list(map(str, xxi2))\n",
        "\n",
        "yyi2 = counts_elements2.tolist()\n",
        "\n",
        "# plt.subplot(2, 1, 1)\n",
        "\n",
        "# print(len(ooi), len(ooi2))\n",
        "\n",
        "# width = 0.35\n",
        "\n",
        "# x = np.arange(len(yyi))\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# rects1 = ax.bar(ooi, yyi, width, label='Men')\n",
        "# rects2 = ax.bar(ooi2, yyi2, width, label='Women')\n",
        "\n",
        "# ax.set_ylabel('Scores')\n",
        "# ax.set_title('Scores by group and gender')\n",
        "# ax.set_xticks(x, yyi)\n",
        "# ax.legend()\n",
        "\n",
        "# ax.bar_label(rects1, padding=3)\n",
        "# ax.bar_label(rects2, padding=3)\n",
        "\n",
        "\n",
        "# p2=plt.barh(ooi2,yyi2)\n",
        "\n",
        "# plt.xlim(0,4000)\n",
        "# plt.xlabel('Number of paraphrase pairs')\n",
        "# plt.ylabel(\"PINC Score\")\n",
        "# addlabels(ooi2,yyi2)\n",
        "\n",
        "\n",
        "# plt.subplot(2,1,2)\n",
        "# p1 = plt.barh(ooi,yyi)\n",
        "\n",
        "# plt.xlabel('Number of non paraphrase pairs')\n",
        "# plt.ylabel(\"PINC Score\")\n",
        "# plt.xlim(0,4000)\n",
        "\n",
        "# addlabels(ooi,yyi)\n",
        "\n",
        "x = np.arange(11)\n",
        "y1 = yyi\n",
        "y2 = yyi2\n",
        "width = 0.4\n",
        "plt.xlabel('Number of pairs')\n",
        "plt.ylabel(\"PINC Score\")\n",
        "plt.yticks(x, ooi)\n",
        "plt.barh(x-0.2, y1, width)\n",
        "plt.barh(x+0.2, y2, width)\n",
        "\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "plt.savefig('pinc.png')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "vtRiKu23yXjz",
        "outputId": "32187799-b131-4a09-a574-198828266f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4c083916bf67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0msentence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPINC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mtot_score2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-55534aca4b40>\u001b[0m in \u001b[0;36mPINC\u001b[0;34m(source, candidate, N)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPINC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokenize_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtokenize_candidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_candidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twokenize/twokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;31m# Assume 'text' has no HTML escaping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msimpleTokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezeWhitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twokenize/twokenize.py\u001b[0m in \u001b[0;36msqueezeWhitespace\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# \"foo   bar \" => \"foo bar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueezeWhitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWhitespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;31m# Final pass tokenization based on special patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "kjNcCKl92bRL",
        "outputId": "f26fc930-dfb8-4bd8-93b8-1e171c4f43e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-41b83049ff36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 314\u001b[0;31m                                  \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arr'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(11)\n",
        "y1 = yyi\n",
        "y2 = yyi2\n",
        "width = 0.4\n",
        "\n",
        "# plot data in grouped manner of bar type\n",
        "\n",
        "plt.xlabel('Number of pairs')\n",
        "plt.ylabel(\"PINC Score\")\n",
        "plt.yticks(x, ooi)\n",
        "plt.barh(x-0.2, y1, width)\n",
        "plt.barh(x+0.2, y2, width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "tQBZe1cx1zKb",
        "outputId": "6cee2c79-2689-423a-eab8-eb52f82f583a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-94e3cd8fbe3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myyi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myyi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yyi' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Mw2T83RgrU1i",
        "outputId": "9000d8dd-f406-4d75-b663-20e3e7a5bc1d"
      },
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method7\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "plt.figure(figsize = (7, 8))\n",
        "\n",
        "# plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "total_rows = dataset.shape[0]\n",
        "\n",
        "ref_list = list()\n",
        "cand_list = list()\n",
        "Sentence_length = list()\n",
        "count = 0\n",
        "tot_score2 = list()\n",
        "\n",
        "\n",
        "total_rows2 = dataset2.shape[0]\n",
        "\n",
        "ref_list2 = list()\n",
        "cand_list2 = list()\n",
        "Sentence_length2 = list()\n",
        "count2 = 0\n",
        "tot_score3 = list()\n",
        "\n",
        "\n",
        "for i in range(total_rows):\n",
        "  sentence = dataset[\"sentence1\"][i]\n",
        "  sentence2 = dataset[\"sentence2\"][i]\n",
        "  score=PINC(sentence,sentence2,3)\n",
        "  tot_score2.append(score)\n",
        "  count += 1\n",
        "\n",
        "for i in range(total_rows2):\n",
        "\n",
        "  sentence3 = dataset2[\"sentence1\"][i]\n",
        "  sentence4 = dataset2[\"sentence2\"][i]\n",
        "  score2=PINC(sentence3,sentence4,3)\n",
        "  tot_score3.append(score2)\n",
        "  count2 += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N = len(tot_score2)\n",
        "print(\"Length=  \", N)\n",
        "\n",
        "print(len(tot_score2))\n",
        "tot_score2=[i for i in tot_score2 if i != 'illegal']\n",
        "\n",
        "print(len(tot_score2))\n",
        "[float(i) for i in tot_score2 if type(i) == str]\n",
        "\n",
        "\n",
        "b=np.round(tot_score2,1)\n",
        "print(b)\n",
        "\n",
        "unique_elements, counts_elements = np.unique(b, return_counts=True)\n",
        "print(\"Frequency of unique values of the said array:\")\n",
        "\n",
        "print(counts_elements)\n",
        "print(unique_elements)\n",
        "xxi = unique_elements.tolist()\n",
        "print(xxi)\n",
        "ooi = list(map(str, xxi))\n",
        "\n",
        "yyi = counts_elements.tolist()\n",
        "print(ooi)\n",
        "ooi.insert(0,'0.1')\n",
        "ooi.insert(0,'0.0')\n",
        "print(yyi)\n",
        "yyi.insert(0,0)\n",
        "yyi.insert(0,0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "N2 = len(tot_score3)\n",
        "print(\"Length=  \", N2)\n",
        "\n",
        "print(len(tot_score3))\n",
        "tot_score3=[i for i in tot_score3 if i != 'illegal']\n",
        "\n",
        "print(len(tot_score3))\n",
        "[float(i) for i in tot_score3 if type(i) == str]\n",
        "\n",
        "\n",
        "b2=np.round(tot_score3,1)\n",
        "print(b2)\n",
        "\n",
        "unique_elements2, counts_elements2 = np.unique(b2, return_counts=True)\n",
        "print(\"Frequency of unique values of the said array:\")\n",
        "\n",
        "print(counts_elements2)\n",
        "print(unique_elements2)\n",
        "xxi2 = unique_elements2.tolist()\n",
        "print(xxi2)\n",
        "ooi2 = list(map(str, xxi2))\n",
        "\n",
        "yyi2 = counts_elements2.tolist()\n",
        "\n",
        "\n",
        "# p1=plt.bar(ooi,yyi, color='darkblue', width=0.5, label='non paraphrase pairs')\n",
        "# plt.subplots(121)\n",
        "# plt.title('PINC Score')\n",
        "plt.subplot(2, 1, 1)\n",
        "# p2=plt.bar(ooi2,yyi2, color='cyan', width=0.5, label='paraphrase pairs')\n",
        "# print('yy2 ',yyi2)\n",
        "\n",
        "p2=plt.barh(ooi2,yyi2)\n",
        "\n",
        "# plt.ylabel('international')\n",
        "plt.xlim(0,4000)\n",
        "plt.xlabel('Number of paraphrase pairs')\n",
        "plt.ylabel(\"PINC Score\")\n",
        "addlabels(ooi2,yyi2)\n",
        "\n",
        "\n",
        "# plt.pie(yyi2, labels=ooi2)\n",
        "# plt.ylabel('paraphrase pairs')\n",
        "# plt.ylim(0,2300)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "# p1=plt.barh(ooi,yyi, color='darkblue', width=0.5, label='non paraphrase pairs')\n",
        "p1 = plt.barh(ooi,yyi)\n",
        "\n",
        "plt.xlabel('Number of non paraphrase pairs')\n",
        "plt.ylabel(\"PINC Score\")\n",
        "plt.xlim(0,4000)\n",
        "# p1.rotate(90)\n",
        "# plt.text(x, y, s, fontsize=12)\n",
        "addlabels(ooi,yyi)\n",
        "# plt.pie(yyi, labels=ooi)\n",
        "# plt.ylim(0,1200)\n",
        "# p1=plt.bar(ind,paraphrase,width,color='darkblue')\n",
        "# p2=plt.bar(ind,nonparaphrase,width,color='red',bottom=paraphrase)\n",
        "# plt.suptitle('PINC Score')\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "plt.savefig('pinc.png')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(2, 2, 3)\n",
        "# # p2=plt.bar(ooi2,yyi2, color='cyan', width=0.5, label='paraphrase pairs')\n",
        "# # print('yy2 ',yyi2)\n",
        "# plt.ylabel('sports')\n",
        "# plt.pie(yyi2, labels=ooi2)\n",
        "# # plt.ylabel('paraphrase pairs')\n",
        "# # plt.ylim(0,2300)\n",
        "# plt.subplot(2,2,4)\n",
        "# # p1=plt.bar(ooi,yyi, color='darkblue', width=0.5, label='non paraphrase pairs')\n",
        "# plt.ylabel('national')\n",
        "# plt.pie(yyi, labels=ooi)\n",
        "# plt.ylim(0,1200)\n",
        "# p1=plt.bar(ind,paraphrase,width,color='darkblue')\n",
        "# p2=plt.bar(ind,nonparaphrase,width,color='red',bottom=paraphrase)\n",
        "# plt.suptitle('PINC Score')\n",
        "\n",
        "# average = statistics.mean(tot_score2)\n",
        "# plt.xticks(('0.1','0.2','0.3', '0.4','0.5', '0.6','0.7', '0.8','0.9', '1.0'),fontsize=18)\n",
        "# # plt.scatter(a,b,alpha=0.5)\n",
        "\n",
        "# print(average*100)\n",
        "\n",
        "# plt.barh(ooi,yyi)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d79596cb8a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0msentence2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mscore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPINC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mtot_score2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-55534aca4b40>\u001b[0m in \u001b[0;36mPINC\u001b[0;34m(source, candidate, N)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPINC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtokenize_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtokenize_candidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_candidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twokenize/twokenize.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;31m# Assume 'text' has no HTML escaping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msimpleTokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqueezeWhitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/twokenize/twokenize.py\u001b[0m in \u001b[0;36msqueezeWhitespace\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# \"foo   bar \" => \"foo bar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueezeWhitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWhitespace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;31m# Final pass tokenization based on special patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHTet03Pe8PS"
      },
      "source": [
        "\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "smoothie = SmoothingFunction().method7\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "\n",
        "total_rows = dataset.shape[0]\n",
        "\n",
        "\n",
        "# Uncomment\n",
        "# print(\"Total Rows\",total_rows)\n",
        "\n",
        "ref_list = list()\n",
        "\n",
        "cand_list = list()\n",
        "\n",
        "Sentence_length = list()\n",
        "\n",
        "count = 0\n",
        "\n",
        "tot_score2 = list()\n",
        "\n",
        "\n",
        "for i in range(total_rows):\n",
        "\n",
        "#   # print(count)\n",
        "  sentence = dataset[\"sentence1\"][i]\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "  sentence = sentence.replace('।', '')\n",
        "  ref = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence)\n",
        "\n",
        "\n",
        "  ref1 = [ref]\n",
        "#   # print('s1: Reference' ,ref1)\n",
        "  # len1 = len(ref)\n",
        "#   # print('Length Candidate_1: ', len1)\n",
        "#   # #Sentence_length.append(len1)\n",
        "\n",
        "  sentence2 = dataset[\"sentence2\"][i]\n",
        "  sentence2 = sentence2.translate(str.maketrans('', '', string.punctuation))\n",
        "  sentence2 = sentence2.replace('।', '')\n",
        "  cand = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence2)\n",
        "\n",
        "\n",
        "#   # print('s2: Candidate', cand)\n",
        "#   # #len2 = len(cand)\n",
        "#   # print('Length Candidate_2: ', len(cand))\n",
        "#   # #Sentence_length.append(len1)\n",
        "\n",
        "  # score1=PINC(sentence,sentence2,2)\n",
        "#   # print(type(score1))\n",
        "  score1 = sentence_bleu(ref1, cand, smoothing_function=smoothie)\n",
        "\n",
        "  # print('ref1 ', ref1)\n",
        "  # print('cand ',cand)\n",
        "\n",
        "  # Uncomment\n",
        "  # print('Score1 ', score1)\n",
        "\n",
        "\n",
        "  sentence = dataset[\"sentence2\"][i]\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "  sentence = sentence.replace('।', '')\n",
        "  ref = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence)\n",
        "\n",
        "\n",
        "  ref1 = [ref]\n",
        "#   # print('s1: Reference' ,ref1)\n",
        "  # len1 = len(ref)\n",
        "#   # print('Length Candidate_1: ', len1)\n",
        "#   # #Sentence_length.append(len1)\n",
        "\n",
        "  sentence2 = dataset[\"sentence1\"][i]\n",
        "  sentence2 = sentence2.translate(str.maketrans('', '', string.punctuation))\n",
        "  sentence2 = sentence2.replace('।', '')\n",
        "  cand = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence2)\n",
        "\n",
        "\n",
        "  score3 = sentence_bleu(ref1, cand, smoothing_function=smoothie)\n",
        "\n",
        "  # print('ref2 ', ref1)\n",
        "  # print('cand2 ',cand)\n",
        "\n",
        "  # Uncomment\n",
        "  # print('Score3 ', score3)\n",
        "\n",
        "  # sentence3 = dataset[\"sentence1\"][i]\n",
        "  # sentence3 = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "  # sentence3 = sentence.replace('।', '')\n",
        "  # ref2 = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence3)\n",
        "\n",
        "\n",
        "  # ref3 = [ref2]\n",
        "\n",
        "\n",
        "#   # print('s1: Reference' ,ref1)\n",
        "  # len1 = len(ref)\n",
        "#   # print('Length Candidate_1: ', len1)\n",
        "#   # #Sentence_length.append(len1)\n",
        "\n",
        "  # sentence4 = dataset[\"sentence1\"][i]\n",
        "  # sentence4 = sentence2.translate(str.maketrans('', '', string.punctuation))\n",
        "  # sentence4 = sentence2.replace('।', '')\n",
        "  # cand2 = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence4)\n",
        "\n",
        "  # score2 = sentence_bleu(ref3, cand2, smoothing_function=smoothie)\n",
        "\n",
        "  # print('ref3 ', ref3)\n",
        "  # print('cand2 ',cand2)\n",
        "  # print('Score2 ', score2)\n",
        "\n",
        "\n",
        "  avg_s = (score1+score3)/2\n",
        "\n",
        "\n",
        "  # print('Average Score ', avg_s)\n",
        "  tot_score2.append(avg_s)\n",
        "  count += 1\n",
        "\n",
        "# N = len(tot_score2)\n",
        "# print(\"Length=  \", N)\n",
        "\n",
        "# print(len(tot_score2))\n",
        "# tot_score2=[i for i in tot_score2 if i != 'illegal']\n",
        "\n",
        "# print(len(tot_score2))\n",
        "# [float(i) for i in tot_score2 if type(i) == str]\n",
        "\n",
        "# print(len(tot_score2))\n",
        "\n",
        "# print('FInal',len(tot_score2))\n",
        "\n",
        "# for i in range(len(tot_score2)):\n",
        "#   if(type(tot_score2[i]==str)):\n",
        "#     tot_score2[i]=float(tot_score2[i])\n",
        "\n",
        "# print('FInal 2.0 ',len(tot_score2))\n",
        "\n",
        "# tot_score2=[item for item in list if type(item) is not str]\n",
        "# tot_score2.remove('illegal')\n",
        "# for i in range(len(tot_score2)):\n",
        "#   if(type(tot_score2[i]==str)):\n",
        "#     print(tot_score2[i])\n",
        "\n",
        "\n",
        "# b=np.round(tot_score2,1)\n",
        "# print(b)\n",
        "# # b.replace()\n",
        "\n",
        "# # print(\"737\", tot_score2[737])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(type(b))\n",
        "# a= [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
        "# plt.hist(b, bins = a, rwidth=0.9)\n",
        "# # plt.hist(b, 50, density=True, facecolor='g', alpha=0.75)\n",
        "# # sns.displot(b)\n",
        "# # print(\"Scary\", set(b))\n",
        "\n",
        "# plt.plot(b, 'ro')\n",
        "# rang=(0.0,1.0)\n",
        "# bins=11\n",
        "# plt.hist(b,a,  color = 'green', rwidth=0.8)\n",
        "\n",
        "\n",
        "# print(len(b))\n",
        "# print(b)\n",
        "# print(set(b))\n",
        "\n",
        "# plt.pie(b)\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# # print(np.where(b==1.1))\n",
        "# # sns.distplot(b,bins=a, kde=False,hist_kws={\"align\" : \"left\"})\n",
        "# # 737, 1202, 1215, 1246\n",
        "# print(Sentence_length)\n",
        "# print(sum(Sentence_length)/len(Sentence_length))\n",
        "\n",
        "dataset.insert(2, 'BLEU_Score', tot_score2)\n",
        "\n",
        "average = statistics.mean(tot_score2)\n",
        "\n",
        "# # plt.scatter(a,b,alpha=0.5)\n",
        "\n",
        "print(average*100)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIJEFQrAgTSn"
      },
      "source": [
        "dataset\n",
        "\n",
        "dataset.to_csv('After_Adding_BLEU_whole_dataset.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0wnLMHdUKGP"
      },
      "source": [
        "combined_dataset = pd.concat([dataset,dataset2,dataset3])\n",
        "combined_dataset\n",
        "combined_dataset.to_csv('combined.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c7hGh-xwOLi"
      },
      "source": [
        "**Finding the index of the best value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKEUO2iFo3X_"
      },
      "source": [
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "\n",
        "F1_Score = list()\n",
        "\n",
        "total_rows = dataset.shape[0]\n",
        "\n",
        "f_s = 0\n",
        "\n",
        "Fsc = 0.0\n",
        "\n",
        "dataset\n",
        "label_list = dataset['label'].tolist()\n",
        "\n",
        "while(Fsc<=1.0):\n",
        "\n",
        "  compare_list = list()\n",
        "  for i in range(total_rows):\n",
        "    bleu_score = dataset[\"BLEU_Score\"][i]\n",
        "    if (bleu_score >= Fsc):\n",
        "      filtered_score = 1\n",
        "      # print('Inside')\n",
        "      compare_list.append(filtered_score)\n",
        "    else:\n",
        "      # print('Outside')\n",
        "      filtered_score = 0\n",
        "      compare_list.append(filtered_score)\n",
        "\n",
        "  Fsc += 0.001\n",
        "  f_s = f1_score(label_list,compare_list,average='weighted')\n",
        "  F1_Score.append(f_s)\n",
        "\n",
        "\n",
        "# print(label_list)\n",
        "# print(compare_list)\n",
        "\n",
        "print(F1_Score)\n",
        "print(len(F1_Score))\n",
        "\n",
        "max_value = max(F1_Score)\n",
        "\n",
        "print(max_value)\n",
        "\n",
        "max_index = F1_Score.index(max_value)\n",
        "\n",
        "print(max_index)\n",
        "\n",
        "comp_var = 0.001*max_index\n",
        "\n",
        "# print('F1 Score : ',f1_score(label_list,compare_list))\n",
        "# print('Accuracy : ',accuracy_score(label_list, compare_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rjtste62Oze"
      },
      "source": [
        "**Finding the F1 & Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_GEy4YdkxRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866a09d9-b6ca-4637-c56b-d0a6f3087d1e"
      },
      "source": [
        "dataset\n",
        "\n",
        "# print(comp_var)\n",
        "\n",
        "total_rows = dataset.shape[0]\n",
        "\n",
        "filtered_score = 0\n",
        "\n",
        "compare_list = list()\n",
        "\n",
        "for i in range(total_rows):\n",
        "  bleu_score = dataset[\"BLEU_Score\"][i]\n",
        "  # print(type(bleu_score))\n",
        "  # print('BLEU Score :', bleu_score)\n",
        "\n",
        "  if (bleu_score >= 0.125):\n",
        "    filtered_score = 1\n",
        "    # print('Inside')\n",
        "    compare_list.append(filtered_score)\n",
        "  else:\n",
        "    # print('Outside')\n",
        "    filtered_score = 0\n",
        "    compare_list.append(filtered_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(compare_list)\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score, precision_score, recall_score\n",
        "\n",
        "dataset\n",
        "label_list = dataset['label'].tolist()\n",
        "\n",
        "# print('label list : ',label_list)\n",
        "# print('compare list : ', compare_list)\n",
        "\n",
        "print(classification_report(label_list,compare_list,digits=4))\n",
        "print('Precision', precision_score(label_list,compare_list,average='weighted')*100)\n",
        "print('Recall', recall_score(label_list,compare_list,average='weighted')*100)\n",
        "print('F1 Score : ',f1_score(label_list,compare_list,average='weighted')*100)\n",
        "print('Accuracy : ',accuracy_score(label_list, compare_list)*100)\n",
        "# print('F1 Score : ',f1_score(label_list,compare_list)*100)\n",
        "# print('Accuracy : ',accuracy_score(label_list, compare_list)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1f11c29166ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(comp_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBGjFzXdxWMv"
      },
      "source": [
        "# dataset.insert(5, 'filtered_Score', compare_list)\n",
        "# del dataset['filtered_Score']\n",
        "\n",
        "df = pd.DataFrame(compare_list, columns=[\"predict\"])\n",
        "df.to_csv('bleu_val_pred.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodb0Quh0UDa"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "sentence = dataset[\"sentence1\"][2731]\n",
        "print(sentence)\n",
        "sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "sentence = sentence.replace('।', '')\n",
        "ref = nltk.tokenize.WhitespaceTokenizer().tokenize(sentence)\n",
        "\n",
        "print(ref)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF7MX9YkjV1T"
      },
      "source": [
        "import numpy as np\n",
        "a= [0.288, 0.25, 0.255, 0.256, 0.249, 0.251, 0.245]\n",
        "b = np.round(a,1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpusjjULrJKf"
      },
      "source": [
        "l = ['Alice', 'Bob', 'Charlie', 'Bob', 'Dave', 'Dave', \"Dave\"]\n",
        "print(l)\n",
        "l.remove('Dave')\n",
        "print(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcOH-A7RVqEW"
      },
      "source": [
        "\n",
        "from __future__ import division\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PINC import PINC\n",
        "def readIn_URL_Data(filename):\n",
        "    data = []\n",
        "    trends = set([])\n",
        "\n",
        "    for line in open(filename):\n",
        "        line = line.strip()\n",
        "        # read in training or dev data with labels\n",
        "        if len(line.split('\\t')) == 4:\n",
        "            (origsent, candsent, judge, url) = line.split('\\t')\n",
        "        # read in test data without labels\n",
        "        elif len(line.split('\\t')) == 3:\n",
        "            (origsent, candsent, judge) = line.split('\\t')\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # if origsent == candsent:\n",
        "        #    continue\n",
        "        #features = paraphrase_Das_features(origsent.decode('utf-8'), candsent.decode('utf-8'))\n",
        "\n",
        "        # ignoring the training/test data that has middle label\n",
        "        if judge[0] == '(':  # labelled by Amazon Mechanical Turk in format like \"(2,3)\"\n",
        "            nYes = eval(judge)[0]\n",
        "            pinc_result=PINC(origsent,candsent,N)\n",
        "            if pinc_result!='illegal':\n",
        "                if nYes >= 4:\n",
        "                    amt_label = 'True'\n",
        "                    data.append((pinc_result, amt_label))\n",
        "                elif nYes <= 2:\n",
        "                    amt_label = 'False'\n",
        "                    data.append((pinc_result, amt_label))\n",
        "                # print data[-1]\n",
        "    #for item in data:\n",
        "    #    with open('../data_URL/test.txt', 'a+') as f:\n",
        "    #        f.writelines(item[2].encode('utf-8') + '\\n')\n",
        "    #        f.writelines(item[3].encode('utf-8') + '\\n')\n",
        "    #sys.exit()\n",
        "    return data\n",
        "\n",
        "def readIn_Semeval_Data(filename):\n",
        "\n",
        "    data = []\n",
        "\n",
        "    (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = (None, None, None, None, None, None, None)\n",
        "\n",
        "    for line in open(filename):\n",
        "        line = line.strip()\n",
        "        #read in training or dev data with labels\n",
        "        if len(line.split('\\t')) == 7:\n",
        "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = line.split('\\t')\n",
        "        #read in test data without labels\n",
        "        elif len(line.split('\\t')) == 6:\n",
        "            print 'hello'\n",
        "            (trendid, trendname, origsent, candsent, origsenttag, candsenttag) = line.split('\\t')\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # ignoring the training/test data that has middle label\n",
        "        if judge[0] == '(':  # labelled by Amazon Mechanical Turk in format like \"(2,3)\"\n",
        "            nYes = eval(judge)[0]\n",
        "            if nYes >= 3:\n",
        "                amt_label = 'True'\n",
        "                data.append((PINC(origsent,candsent,N), amt_label))\n",
        "            elif nYes <= 1:\n",
        "                amt_label = 'False'\n",
        "                data.append((PINC(origsent, candsent,N), amt_label))\n",
        "        elif judge[0].isdigit():   # labelled by expert in format like \"2\"\n",
        "            nYes = int(judge[0])\n",
        "            if nYes >= 4:\n",
        "                expert_label = 'True'\n",
        "                data.append((PINC(origsent, candsent,N), expert_label))\n",
        "            elif nYes <= 2:\n",
        "                expert_label = 'False'\n",
        "                data.append((PINC(origsent, candsent,N), expert_label))\n",
        "\n",
        "    '''\n",
        "    for item in data:\n",
        "        with open('../data_Semeval/test.txt', 'a+') as f:\n",
        "            f.writelines(item[2]+'\\n')\n",
        "            f.writelines(item[3]+'\\n')\n",
        "    sys.exit()\n",
        "    '''\n",
        "    return data\n",
        "\n",
        "def readIn_MSRP_Data(filename):\n",
        "    data = []\n",
        "    input_data = []\n",
        "    firstLine = True\n",
        "\n",
        "    for line in open(filename):\n",
        "        line = line.decode('utf-8').strip()\n",
        "        # line = line.decode('ascii', 'ignore')\n",
        "        if firstLine:\n",
        "            firstLine=False\n",
        "            continue\n",
        "        if len(line.split('\\t')) == 5:\n",
        "            (score, ID_1, ID_2, sent1, sent2) = line.split('\\t')\n",
        "            if int(score) == 1:\n",
        "                label = 'True'\n",
        "            else:\n",
        "                label = 'False'\n",
        "            data.append((PINC(sent1,sent2,N), label))\n",
        "    '''\n",
        "    for item in data:\n",
        "        with open('../data_msrp/test.txt', 'a+') as f:\n",
        "            f.writelines(item[0].encode('utf-8')+'\\n')\n",
        "            f.writelines(item[1].encode('utf-8')+'\\n')\n",
        "    sys.exit()\n",
        "\n",
        "    for item in data:\n",
        "        features = paraphrase_Das_features(item[0], item[1])\n",
        "        input_data.append((features, item[2], item[0], item[1]))\n",
        "    '''\n",
        "    return data\n",
        "\n",
        "def readIn_Hindi_Data(dir):\n",
        "\tdata=[]\n",
        "\tlsents = []\n",
        "\trsents = []\n",
        "\tlabels = []\n",
        "\tfor line in open(dir + 'a.toks'):\n",
        "\t\tline = line.strip().decode('utf-8')\n",
        "\t\tlsents.append(line)\n",
        "\tfor line in open(dir + 'b.toks'):\n",
        "\t\tline = line.strip().decode('utf-8')\n",
        "\t\trsents.append(line)\n",
        "\tfor line in open(dir + 'sim.txt'):\n",
        "\t\tif line.strip()=='1':\n",
        "\t\t\tlabels.append('True')\n",
        "\t\telse:\n",
        "\t\t\tlabels.append('False')\n",
        "\tif not len(lsents) == len(rsents) == len(labels):\n",
        "\t\tprint('error!')\n",
        "\t\tsys.exit()\n",
        "\telse:\n",
        "\t\tfor i in range(len(labels)):\n",
        "\t\t\tdata.append((PINC(lsents[i],rsents[i],N), labels[i]))\n",
        "\treturn data\n",
        "\n",
        "if __name__=='__main__':\n",
        "    N=3 #parameter for PINC function\n",
        "    #trainfile='/Users/lan/Documents/research/LEXLATENT/data_msrp/msr_paraphrase_train.txt'\n",
        "    #testfile='/Users/lan/Documents/research/LEXLATENT/data_msrp/msr_paraphrase_test.txt'\n",
        "    #trainfile = '/Users/lan/Documents/research/LEXLATENT/data_Semeval/train.data'\n",
        "    #testfile = '/Users/lan/Documents/research/LEXLATENT/data_Semeval/test.data'\n",
        "    #trainfile='/Users/lan/Documents/research/LEXLATENT/data_URL/Twitter_URL_Corpus_train.txt'\n",
        "    #testfile='/Users/lan/Documents/research/LEXLATENT/data_URL/Twitter_URL_Corpus_test.txt'\n",
        "    trainfile='/Users/lan/Documents/research/LEXLATENT/data_Hindi/train/'\n",
        "    testfile='/Users/lan/Documents/research/LEXLATENT/data_Hindi/test/'\n",
        "    #result=readIn_MSRP_Data(trainfile)\n",
        "    #result=readIn_Semeval_Data(trainfile)\n",
        "    #result=readIn_URL_Data(trainfile)\n",
        "    result=readIn_Hindi_Data(trainfile)\n",
        "    print len(result)\n",
        "    #result+=readIn_MSRP_Data(testfile)\n",
        "    #result+=readIn_Semeval_Data(testfile)\n",
        "    #result+=readIn_URL_Data(testfile)\n",
        "    result += readIn_Hindi_Data(testfile)\n",
        "    print len(result)\n",
        "    M=10 # number of bars\n",
        "    paraphrase=[0,0,0,0,0,0,0,0,0,0]\n",
        "    nonparaphrase=[0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in range(len(result)):\n",
        "        if result[i][1]=='True':\n",
        "            if result[i][0]<1:\n",
        "                paraphrase[np.floor(result[i][0]*M).astype(int)]+=1\n",
        "            else:\n",
        "                paraphrase[9]+=1\n",
        "        else:\n",
        "            if result[i][0]<1:\n",
        "                nonparaphrase[np.floor(result[i][0]*M).astype(int)]+=1\n",
        "            else:\n",
        "                nonparaphrase[9]+=1\n",
        "    a=sum(paraphrase)\n",
        "    print paraphrase\n",
        "    print nonparaphrase\n",
        "    for i in range(len(paraphrase)):\n",
        "        paraphrase[i]=paraphrase[i]*100/a\n",
        "    #percentage = np.true_divide(paraphrase, np.add(paraphrase, nonparaphrase))\n",
        "    #print paraphrase\n",
        "    a=sum(nonparaphrase)\n",
        "    for i in range(len(nonparaphrase)):\n",
        "        nonparaphrase[i]=nonparaphrase[i]*100/a\n",
        "    ind = np.arange(M)\n",
        "    width = 0.7\n",
        "    p1=plt.bar(ind,paraphrase,width,color='darkblue')\n",
        "    p2=plt.bar(ind,nonparaphrase,width,color='red',bottom=paraphrase)\n",
        "    plt.xlabel('PINC Score',fontsize=18)\n",
        "\n",
        "    #plt.ylabel('MSRP')\n",
        "    plt.ylabel('% of Paraphrase Pairs',fontsize=18)\n",
        "    #plt.ylabel('URL')\n",
        "    #plt.title('MSRP Paraphrase Distribution for PINC Score',fontsize=18)\n",
        "    #plt.ylim(0,40)\n",
        "    #plt.yticks([0,5,10,15,20,25,30,35,40,45, 50, 55, 60, 65, 70, 75, 80],('0','5','10','15','20','25','30','35','40'),fontsize=18)\n",
        "    plt.title('PIT-2015 Paraphrase Distribution for PINC Score',fontsize=18)\n",
        "    #plt.title('Twitter URL Paraphrase Distribution for PINC Score',fontsize=18)\n",
        "    plt.xticks(ind + 0.35 , ('0.1','0.2','0.3', '0.4','0.5', '0.6','0.7', '0.8','0.9', '1.0'),fontsize=18)\n",
        "    #plt.yticks(np.arange(0, 1000, 100))\n",
        "    plt.legend((p1[0], p2[0]), ('Paraphrase', 'Nonparaphrase'),loc=2)\n",
        "    #index=0\n",
        "    #for r1, r2 in zip(p1, p2):\n",
        "    #    h1 = r1.get_height()\n",
        "    #    h2 = r2.get_height()\n",
        "    #    plt.text(r1.get_x() + r1.get_width() / 2., h1 + h2, '%.2f' % (percentage[index]), ha='center', va='bottom')\n",
        "    #    index+=1\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DP0KqpJ6vWy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}