{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingFace: GLUE Benchmark",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOta2z66THbXjVcnXUj8Lls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanShaheb34/Bangla-Paraphrase-Identification/blob/master/Shakirul/Scripts/HuggingFace_GLUE_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLgdEImZ_WI1"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUICV43Z_bci"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4OeQK6a_dK_"
      },
      "source": [
        "!cp transformers/examples/text-classification/* ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmeIazOF_nqe"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPnwdaiXYpPp"
      },
      "source": [
        "!python run_glue.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKkdSjWU_9g_",
        "outputId": "95396a0a-a15d-4ee3-c361-95c90d0c4304"
      },
      "source": [
        "!python run_glue.py --model_name_or_path bert-base-multilingual-uncased --do_train --do_eval --train_file /content/dataset/train.csv --test_file /content/dataset/test.csv --validation_file /content/dataset/val.csv --output_dir /content/output --do_predict --num_train_epochs 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-23 19:47:16.105133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/23/2021 19:47:17 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/23/2021 19:47:17 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/output, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan23_19-47-17_7b5a53e6ec23, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=1)\n",
            "01/23/2021 19:47:17 - INFO - __main__ -   load a local file for train: /content/dataset/train.csv\n",
            "01/23/2021 19:47:17 - INFO - __main__ -   load a local file for validation: /content/dataset/val.csv\n",
            "01/23/2021 19:47:17 - INFO - __main__ -   load a local file for test: /content/dataset/test.csv\n",
            "Downloading: 5.33kB [00:00, 6.94MB/s]       \n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset csv/default-5a0e525c80440835 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-5a0e525c80440835/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-5a0e525c80440835/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348126224 acquired on /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508.lock\n",
            "[INFO|file_utils.py:1272] 2021-01-23 19:47:17,727 >> https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3mtk1nqo\n",
            "Downloading: 100% 625/625 [00:00<00:00, 997kB/s]\n",
            "[INFO|file_utils.py:1276] 2021-01-23 19:47:17,742 >> storing https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n",
            "[INFO|file_utils.py:1279] 2021-01-23 19:47:17,742 >> creating metadata file for /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348126224 released on /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508.lock\n",
            "[INFO|configuration_utils.py:445] 2021-01-23 19:47:17,743 >> loading configuration file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n",
            "[INFO|configuration_utils.py:481] 2021-01-23 19:47:17,743 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:445] 2021-01-23 19:47:17,760 >> loading configuration file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n",
            "[INFO|configuration_utils.py:481] 2021-01-23 19:47:17,760 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348258536 acquired on /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b.lock\n",
            "[INFO|file_utils.py:1272] 2021-01-23 19:47:17,784 >> https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp39sy2jlx\n",
            "Downloading: 100% 872k/872k [00:00<00:00, 20.8MB/s]\n",
            "[INFO|file_utils.py:1276] 2021-01-23 19:47:17,850 >> storing https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b\n",
            "[INFO|file_utils.py:1279] 2021-01-23 19:47:17,850 >> creating metadata file for /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348258536 released on /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b.lock\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348258424 acquired on /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96.lock\n",
            "[INFO|file_utils.py:1272] 2021-01-23 19:47:17,880 >> https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7auou7em\n",
            "Downloading: 100% 1.72M/1.72M [00:00<00:00, 26.9MB/s]\n",
            "[INFO|file_utils.py:1276] 2021-01-23 19:47:17,974 >> storing https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96\n",
            "[INFO|file_utils.py:1279] 2021-01-23 19:47:17,974 >> creating metadata file for /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96\n",
            "01/23/2021 19:47:17 - INFO - filelock -   Lock 140173348258424 released on /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96.lock\n",
            "[INFO|tokenization_utils_base.py:1766] 2021-01-23 19:47:17,975 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b\n",
            "[INFO|tokenization_utils_base.py:1766] 2021-01-23 19:47:17,975 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96\n",
            "01/23/2021 19:47:18 - INFO - filelock -   Lock 140174339687088 acquired on /root/.cache/huggingface/transformers/37f730c9dc4fc13ab6bf412fdc0ad936241a39a70628c2d4a85a607ea775b865.a458b2dad7b293099dd815628e032e6c22519889d75f13d6f244dbe068525a56.lock\n",
            "[INFO|file_utils.py:1272] 2021-01-23 19:47:18,046 >> https://huggingface.co/bert-base-multilingual-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxaq2sbwj\n",
            "Downloading: 100% 672M/672M [00:13<00:00, 51.1MB/s]\n",
            "[INFO|file_utils.py:1276] 2021-01-23 19:47:31,462 >> storing https://huggingface.co/bert-base-multilingual-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/37f730c9dc4fc13ab6bf412fdc0ad936241a39a70628c2d4a85a607ea775b865.a458b2dad7b293099dd815628e032e6c22519889d75f13d6f244dbe068525a56\n",
            "[INFO|file_utils.py:1279] 2021-01-23 19:47:31,462 >> creating metadata file for /root/.cache/huggingface/transformers/37f730c9dc4fc13ab6bf412fdc0ad936241a39a70628c2d4a85a607ea775b865.a458b2dad7b293099dd815628e032e6c22519889d75f13d6f244dbe068525a56\n",
            "01/23/2021 19:47:31 - INFO - filelock -   Lock 140174339687088 released on /root/.cache/huggingface/transformers/37f730c9dc4fc13ab6bf412fdc0ad936241a39a70628c2d4a85a607ea775b865.a458b2dad7b293099dd815628e032e6c22519889d75f13d6f244dbe068525a56.lock\n",
            "[INFO|modeling_utils.py:1027] 2021-01-23 19:47:31,462 >> loading weights file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/37f730c9dc4fc13ab6bf412fdc0ad936241a39a70628c2d4a85a607ea775b865.a458b2dad7b293099dd815628e032e6c22519889d75f13d6f244dbe068525a56\n",
            "[WARNING|modeling_utils.py:1135] 2021-01-23 19:47:36,834 >> Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1146] 2021-01-23 19:47:36,834 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 6/6 [00:00<00:00,  7.44ba/s]\n",
            "100% 2/2 [00:00<00:00, 13.69ba/s]\n",
            "100% 2/2 [00:00<00:00, 12.57ba/s]\n",
            "01/23/2021 19:47:38 - INFO - __main__ -   Sample 912 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 45637, 100, 94366, 644, 67566, 639, 26419, 53435, 640, 94617, 618, 11276, 74220, 640, 88787, 19801, 102, 640, 88787, 72930, 94366, 639, 26419, 53435, 648, 23711, 19182, 19469, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'ঢাকা–কলকাতা রুটে ফ্লাইট বন্ধ করলো বিমান', 'sentence2': 'বিমানের কলকাতা ফ্লাইট স্থগিত', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/23/2021 19:47:38 - INFO - __main__ -   Sample 204 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 638, 16393, 54277, 57144, 85844, 11480, 640, 14665, 12946, 660, 20356, 85203, 648, 23281, 44104, 117, 635, 19734, 12067, 29252, 662, 19858, 49753, 43328, 102, 660, 20356, 85203, 648, 23281, 44104, 640, 61243, 47583, 11276, 39994, 635, 19734, 12067, 29252, 92065, 638, 16393, 54277, 57144, 29519, 638, 11480, 96838, 18602, 11480, 663, 31589, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'পদ্মা সেতুতে বসল ৩৮তম স্প্যান, দৃশ্যমান ৫৭০০ মিটার', 'sentence2': '৩৮তম স্প্যান বসানোর মাধ্যমে দৃশ্যমান হলো পদ্মা সেতুর পৌণে ৬ কিলোমিটার', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "01/23/2021 19:47:38 - INFO - __main__ -   Sample 2253 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 607, 25768, 51050, 608, 28536, 18093, 23281, 23281, 13493, 102, 608, 28536, 18701, 29028, 52271, 11799, 18093, 23281, 23281, 13493, 118, 607, 23281, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'sentence1': 'অপুর সঙ্গে আবার বাপ্পী', 'sentence2': 'আবারও এক হচ্ছেন বাপ্পী-অপু', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:442] 2021-01-23 19:47:52,534 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:442] 2021-01-23 19:47:52,535 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:791] 2021-01-23 19:47:52,539 >> ***** Running training *****\n",
            "[INFO|trainer.py:792] 2021-01-23 19:47:52,539 >>   Num examples = 5173\n",
            "[INFO|trainer.py:793] 2021-01-23 19:47:52,539 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:794] 2021-01-23 19:47:52,539 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:795] 2021-01-23 19:47:52,539 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:796] 2021-01-23 19:47:52,539 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:797] 2021-01-23 19:47:52,539 >>   Total optimization steps = 6470\n",
            "{'loss': 0.4711, 'learning_rate': 4.6136012364760434e-05, 'epoch': 0.77}\n",
            "  8% 500/6470 [01:59<25:42,  3.87it/s][INFO|trainer.py:1344] 2021-01-23 19:49:51,851 >> Saving model checkpoint to /content/output/checkpoint-500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 19:49:51,853 >> Configuration saved in /content/output/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 19:49:54,589 >> Model weights saved in /content/output/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 0.3804, 'learning_rate': 4.2272024729520865e-05, 'epoch': 1.55}\n",
            " 15% 1000/6470 [04:16<23:03,  3.95it/s][INFO|trainer.py:1344] 2021-01-23 19:52:08,925 >> Saving model checkpoint to /content/output/checkpoint-1000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 19:52:08,927 >> Configuration saved in /content/output/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 19:52:11,365 >> Model weights saved in /content/output/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 0.3335, 'learning_rate': 3.84080370942813e-05, 'epoch': 2.32}\n",
            " 23% 1500/6470 [06:31<21:04,  3.93it/s][INFO|trainer.py:1344] 2021-01-23 19:54:24,393 >> Saving model checkpoint to /content/output/checkpoint-1500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 19:54:24,394 >> Configuration saved in /content/output/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 19:54:26,700 >> Model weights saved in /content/output/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 0.2905, 'learning_rate': 3.4544049459041734e-05, 'epoch': 3.09}\n",
            " 31% 2000/6470 [08:47<18:57,  3.93it/s][INFO|trainer.py:1344] 2021-01-23 19:56:40,169 >> Saving model checkpoint to /content/output/checkpoint-2000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 19:56:40,170 >> Configuration saved in /content/output/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 19:56:42,900 >> Model weights saved in /content/output/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 0.2439, 'learning_rate': 3.0680061823802165e-05, 'epoch': 3.86}\n",
            " 39% 2500/6470 [11:02<16:40,  3.97it/s][INFO|trainer.py:1344] 2021-01-23 19:58:55,346 >> Saving model checkpoint to /content/output/checkpoint-2500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 19:58:55,347 >> Configuration saved in /content/output/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 19:58:57,851 >> Model weights saved in /content/output/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 0.1821, 'learning_rate': 2.6816074188562596e-05, 'epoch': 4.64}\n",
            " 46% 3000/6470 [13:17<14:53,  3.88it/s][INFO|trainer.py:1344] 2021-01-23 20:01:10,009 >> Saving model checkpoint to /content/output/checkpoint-3000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:01:10,011 >> Configuration saved in /content/output/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:01:12,768 >> Model weights saved in /content/output/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 0.1614, 'learning_rate': 2.295208655332303e-05, 'epoch': 5.41}\n",
            " 54% 3500/6470 [15:32<12:16,  4.03it/s][INFO|trainer.py:1344] 2021-01-23 20:03:24,866 >> Saving model checkpoint to /content/output/checkpoint-3500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:03:24,868 >> Configuration saved in /content/output/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:03:27,401 >> Model weights saved in /content/output/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 0.1111, 'learning_rate': 1.9088098918083462e-05, 'epoch': 6.18}\n",
            " 62% 4000/6470 [17:46<10:18,  3.99it/s][INFO|trainer.py:1344] 2021-01-23 20:05:38,961 >> Saving model checkpoint to /content/output/checkpoint-4000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:05:38,963 >> Configuration saved in /content/output/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:05:41,530 >> Model weights saved in /content/output/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 0.0701, 'learning_rate': 1.5224111282843895e-05, 'epoch': 6.96}\n",
            " 70% 4500/6470 [20:00<08:12,  4.00it/s][INFO|trainer.py:1344] 2021-01-23 20:07:52,962 >> Saving model checkpoint to /content/output/checkpoint-4500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:07:52,964 >> Configuration saved in /content/output/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:07:55,527 >> Model weights saved in /content/output/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 0.0479, 'learning_rate': 1.1360123647604328e-05, 'epoch': 7.73}\n",
            " 77% 5000/6470 [22:14<06:07,  4.00it/s][INFO|trainer.py:1344] 2021-01-23 20:10:06,731 >> Saving model checkpoint to /content/output/checkpoint-5000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:10:06,732 >> Configuration saved in /content/output/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:10:09,401 >> Model weights saved in /content/output/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 0.029, 'learning_rate': 7.496136012364761e-06, 'epoch': 8.5}\n",
            " 85% 5500/6470 [24:28<04:02,  4.00it/s][INFO|trainer.py:1344] 2021-01-23 20:12:20,697 >> Saving model checkpoint to /content/output/checkpoint-5500\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:12:20,698 >> Configuration saved in /content/output/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:12:23,278 >> Model weights saved in /content/output/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 0.0328, 'learning_rate': 3.6321483771251936e-06, 'epoch': 9.27}\n",
            " 93% 6000/6470 [26:41<01:57,  3.99it/s][INFO|trainer.py:1344] 2021-01-23 20:14:34,477 >> Saving model checkpoint to /content/output/checkpoint-6000\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:14:34,478 >> Configuration saved in /content/output/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:14:37,236 >> Model weights saved in /content/output/checkpoint-6000/pytorch_model.bin\n",
            "100% 6470/6470 [28:48<00:00,  4.42it/s][INFO|trainer.py:953] 2021-01-23 20:16:40,651 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1728.1119, 'train_samples_per_second': 3.744, 'epoch': 10.0}\n",
            "100% 6470/6470 [28:48<00:00,  3.74it/s]\n",
            "[INFO|trainer.py:1344] 2021-01-23 20:16:40,696 >> Saving model checkpoint to /content/output\n",
            "[INFO|configuration_utils.py:300] 2021-01-23 20:16:40,698 >> Configuration saved in /content/output/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-23 20:16:43,221 >> Model weights saved in /content/output/pytorch_model.bin\n",
            "01/23/2021 20:16:43 - INFO - __main__ -   ***** Train results *****\n",
            "01/23/2021 20:16:43 - INFO - __main__ -     epoch = 10.0\n",
            "01/23/2021 20:16:43 - INFO - __main__ -     train_runtime = 1728.1119\n",
            "01/23/2021 20:16:43 - INFO - __main__ -     train_samples_per_second = 3.744\n",
            "01/23/2021 20:16:43 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:442] 2021-01-23 20:16:43,381 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:1536] 2021-01-23 20:16:43,381 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1537] 2021-01-23 20:16:43,381 >>   Num examples = 1109\n",
            "[INFO|trainer.py:1538] 2021-01-23 20:16:43,382 >>   Batch size = 8\n",
            "100% 139/139 [00:10<00:00, 13.23it/s]\n",
            "01/23/2021 20:16:53 - INFO - __main__ -   ***** Eval results None *****\n",
            "01/23/2021 20:16:53 - INFO - __main__ -     epoch = 10.0\n",
            "01/23/2021 20:16:53 - INFO - __main__ -     eval_accuracy = 0.8999098539352417\n",
            "01/23/2021 20:16:53 - INFO - __main__ -     eval_loss = 0.8005388975143433\n",
            "01/23/2021 20:16:53 - INFO - __main__ -     eval_runtime = 10.5332\n",
            "01/23/2021 20:16:53 - INFO - __main__ -     eval_samples_per_second = 105.286\n",
            "01/23/2021 20:16:53 - INFO - __main__ -   *** Test ***\n",
            "[INFO|trainer.py:442] 2021-01-23 20:16:53,935 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, sentence2.\n",
            "[INFO|trainer.py:1536] 2021-01-23 20:16:53,936 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:1537] 2021-01-23 20:16:53,936 >>   Num examples = 1111\n",
            "[INFO|trainer.py:1538] 2021-01-23 20:16:53,936 >>   Batch size = 8\n",
            "100% 139/139 [00:10<00:00, 13.14it/s]01/23/2021 20:17:04 - INFO - __main__ -   ***** Test results None *****\n",
            "100% 139/139 [00:10<00:00, 13.20it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8kKO7V5QFTJ"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TucX6ZqsMV2A"
      },
      "source": [
        "test_results_path = '/content/output/test_results_None.txt'\n",
        "test_data_path = '/content/dataset/test.csv'\n",
        "\n",
        "test_results_df = pd.read_csv(test_results_path, sep='\\t')\n",
        "test_df = pd.read_csv(test_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ii6ZTfaQabF",
        "outputId": "2f6311a1-9e07-4805-860d-6f04e5161f51"
      },
      "source": [
        "print(metrics.classification_report(test_df[\"label\"], test_results_df['prediction']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92       683\n",
            "           1       0.86      0.87      0.87       428\n",
            "\n",
            "    accuracy                           0.90      1111\n",
            "   macro avg       0.89      0.89      0.89      1111\n",
            "weighted avg       0.90      0.90      0.90      1111\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}