{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetune BERT for BNPP","metadata":{"id":"H23Kv1nrIPVg"}},{"cell_type":"markdown","source":"# Configuration\n\nOnly configure these 2 code sections ","metadata":{}},{"cell_type":"code","source":"PRETRAINED_MODEL_NAME = \"bert-base-multilingual-cased\"\n# PRETRAINED_MODEL_NAME = \"csebuetnlp/banglabert\"\nBATCH_SIZE = 8\nMAX_LEN = 512\nEPOCHS = 20\nLEARNING_RATE = 1e-5\nMODEL_PATH = f'{PRETRAINED_MODEL_NAME.replace(\"/\", \"-\")}_{BATCH_SIZE}_{LEARNING_RATE}.bin'\n\n\"\"\"\nOther Huggingface Models\ncsebuetnlp/banglabert\nsagorsarker/bangla-bert-base\nbert-base-multilingual-uncased\nbert-base-multilingual-cased\ndistilbert-base-multilingual-cased\nneuropark/sahajBERT\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset = \"name_of_the_dataset_to_be_trained_on\"\npath_to_dataset = \"/path/to/dataset\"\ntrain_file_path = f\"{path_to_dataset}/training_dataset.csv\"\ntest_file_path = f\"{path_to_dataset}/testing_dataset.csv\"\nval_file_path = f\"{path_to_dataset}/TaPaCo_val.csv\"\ntesting_files = [\n    \"BnPC_test_2_label\",\n    \"TaPaCo_test\",\n    \"indic_test_bnpc_1\",\n    \"indic_test_bnpc_2\",\n    \"indic_test_bnpc_3\",    \n    \"BUET_test_bnpc_1\",\n    \"BUET_test_bnpc_2\",\n    \"BUET_test_bnpc_3\"\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Installing Necessary Packages","metadata":{"id":"Z3D2lGZKxAQ9"}},{"cell_type":"code","source":"! pip install transformers","metadata":{"id":"XhFfARwbrfY2","execution":{"iopub.status.busy":"2023-10-15T06:12:08.615535Z","iopub.execute_input":"2023-10-15T06:12:08.616551Z","iopub.status.idle":"2023-10-15T06:12:21.513394Z","shell.execute_reply.started":"2023-10-15T06:12:08.616516Z","shell.execute_reply":"2023-10-15T06:12:21.512012Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Downloading Dataset","metadata":{"id":"UYNwv9VUxDyR"}},{"cell_type":"markdown","source":"## Importing Necessary Packages and Switching to GPU if Available","metadata":{"id":"wol1vNCaxL7L"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport transformers\nimport torch\nimport re\nimport pickle\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\n\nfrom transformers import AutoTokenizer, AutoModel \n\nfrom collections import defaultdict\n\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"fnCknHAcrmal","outputId":"35f23edf-7972-415f-85e7-83085a686708","execution":{"iopub.status.busy":"2023-10-15T06:12:21.515172Z","iopub.execute_input":"2023-10-15T06:12:21.515579Z","iopub.status.idle":"2023-10-15T06:12:28.092130Z","shell.execute_reply.started":"2023-10-15T06:12:21.515546Z","shell.execute_reply":"2023-10-15T06:12:28.091398Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cpu')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Loader","metadata":{"id":"V8_fBOWxQG1e"}},{"cell_type":"code","source":"# Function to hide numbers from a sentence\ndef hide_numbers(df, token=' [CC] '):\n  df.sentence1 = [re.sub(r\"[^ঀ-ৣ ]\", '', s).strip() for s in df.sentence1]\n  df.sentence2 = [re.sub(r\"[^ঀ-ৣ ]\", '', s).strip() for s in df.sentence2]\n  return df\n\n# Load dataset from file\ndef load_data(train_path, test_path, val_path):\n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    val_df = pd.read_csv(val_path)\n\n    train_df = hide_numbers(train_df)\n    test_df = hide_numbers(test_df)\n    val_df = hide_numbers(val_df)\n\n    return train_df, test_df, val_df\n\n# Dataset helper class\nclass BNPPDataset(Dataset):\n    def __init__(self, sentence1, sentence2, labels, tokenizer, max_len):\n        self.sentence1 = sentence1\n        self.sentence2 = sentence2\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, item):\n        s1 = str(self.sentence1[item])\n        s2 = str(self.sentence2[item])\n        label = self.labels[item]\n\n        encoding = self.tokenizer.encode_plus(\n            s1, s2,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'sentence1': s1,\n            'sentence2': s2,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Creating data loaders\ndef create_data_loader(df, tokenizer, max_len, batch_size, shuffle=False):\n    ds = BNPPDataset(\n        sentence1=df.sentence1.to_numpy(),\n        sentence2=df.sentence2.to_numpy(),\n        labels=df.label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len,\n    )\n\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=2,\n        shuffle=shuffle\n    )\n\n# Exporting data loaders\ndef get_dataloader(train_path, test_path, val_path, tokenizer, MAX_LEN, BATCH_SIZE):\n    train_df, test_df, val_df = load_data(train_path, test_path, val_path)\n\n    train_data_loader = create_data_loader(\n        train_df, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=True)\n    test_data_loader = create_data_loader(\n        test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n    val_data_loader = create_data_loader(\n        val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n\n    return train_data_loader, test_data_loader, val_data_loader","metadata":{"id":"1jGfR1VPQCH6","execution":{"iopub.status.busy":"2023-10-15T06:12:28.094862Z","iopub.execute_input":"2023-10-15T06:12:28.096825Z","iopub.status.idle":"2023-10-15T06:12:28.111055Z","shell.execute_reply.started":"2023-10-15T06:12:28.096780Z","shell.execute_reply":"2023-10-15T06:12:28.109937Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Building the Model","metadata":{"id":"_TdN3r-TQQdJ"}},{"cell_type":"code","source":"class ParaphraseIdentifier(nn.Module):\n    def __init__(self, model_name):\n        super(ParaphraseIdentifier, self).__init__()\n        self.bert = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, 2)\n        self.activation = nn.LogSoftmax(dim=-1)\n\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False\n        )\n        drop_output = self.drop(pooled_output)\n        linear_output = self.linear(drop_output)\n        out = self.activation(linear_output)\n\n        return out","metadata":{"id":"3dosS5sTQM_E","execution":{"iopub.status.busy":"2023-10-15T06:12:28.112569Z","iopub.execute_input":"2023-10-15T06:12:28.113065Z","iopub.status.idle":"2023-10-15T06:12:28.134001Z","shell.execute_reply.started":"2023-10-15T06:12:28.113022Z","shell.execute_reply":"2023-10-15T06:12:28.133112Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Training and Testing Helper Functions","metadata":{"id":"hWVkIY58QVyI"}},{"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        optimizer.zero_grad()\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"labels\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        preds = outputs.argmax(-1).tolist()\n        loss = loss_fn(outputs, labels)\n\n        cp = 0\n        for i, v in enumerate(labels.tolist()):\n            if v == preds[i]:\n                cp += 1\n\n        correct_predictions += cp\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n    return correct_predictions / n_examples, np.mean(losses)\n\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            labels = d[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            preds = outputs.argmax(-1).tolist()\n            loss = loss_fn(outputs, labels)\n\n            cp = 0\n            for i, v in enumerate(labels.tolist()):\n                if v == preds[i]:\n                    cp += 1\n\n            correct_predictions += cp\n            losses.append(loss.item())\n\n    return correct_predictions / n_examples, np.mean(losses)","metadata":{"id":"olv7QrctQSic","execution":{"iopub.status.busy":"2023-10-15T06:12:28.135634Z","iopub.execute_input":"2023-10-15T06:12:28.136116Z","iopub.status.idle":"2023-10-15T06:12:28.149590Z","shell.execute_reply.started":"2023-10-15T06:12:28.136088Z","shell.execute_reply":"2023-10-15T06:12:28.148335Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Variables to Configure the Dataset","metadata":{"id":"mSeJlEvbQam1"}},{"cell_type":"markdown","source":"## Training","metadata":{"id":"dYspCTMTQfPp"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\ntrain_data_loader, test_data_loader, val_data_loader = get_dataloader(\n    train_file_path, test_file_path, val_file_path, tokenizer, MAX_LEN, BATCH_SIZE)\ntrain_df, test_df, val_df = load_data(train_file_path, test_file_path, val_file_path)\nmodel = nn.DataParallel(ParaphraseIdentifier(PRETRAINED_MODEL_NAME)).to(device)\n\noptimizer = transformers.AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = transformers.get_linear_schedule_with_warmup(\n    optimizer, 0, total_steps)\nloss_fn = nn.NLLLoss().to(device)\n\nhistory = defaultdict(list)\nbest_accuracy = 0\nbest_epoch = 0\n\nfor epoch in range(EPOCHS):\n\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n        model, train_data_loader, loss_fn, optimizer, device, scheduler, len(train_df))\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model, val_data_loader, loss_fn, device, len(val_df))\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), MODEL_PATH)\n        print(f'Saved model with accuracy {val_acc} on: {MODEL_PATH}')\n        best_accuracy = val_acc\n        best_epoch = epoch\n\n    if epoch - best_epoch > 5:\n        print('Not good accuracy for last 5 epochs. Cancelling...')\n        break","metadata":{"id":"N83aLqZCQcUu","execution":{"iopub.status.busy":"2023-10-11T02:30:20.134895Z","iopub.execute_input":"2023-10-11T02:30:20.135753Z","iopub.status.idle":"2023-10-11T02:30:54.855045Z","shell.execute_reply.started":"2023-10-11T02:30:20.135724Z","shell.execute_reply":"2023-10-11T02:30:54.850052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Pkl_Filename = f'{training_dataset}_{PRETRAINED_MODEL_NAME.replace(\"/\", \"-\")}_{BATCH_SIZE}_{LEARNING_RATE}.pkl'  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T02:30:54.858272Z","iopub.status.idle":"2023-10-11T02:30:54.860529Z","shell.execute_reply.started":"2023-10-11T02:30:54.860270Z","shell.execute_reply":"2023-10-11T02:30:54.860306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Training History","metadata":{"id":"cJz77AiWyFO7"}},{"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","metadata":{"id":"BbDcQenrVJRW","outputId":"75b2cc0d-3303-4e6a-9277-14c9f99d622d","execution":{"iopub.status.busy":"2023-10-11T02:30:54.864292Z","iopub.status.idle":"2023-10-11T02:30:54.866526Z","shell.execute_reply.started":"2023-10-11T02:30:54.866265Z","shell.execute_reply":"2023-10-11T02:30:54.866291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running test on test dataset","metadata":{"id":"52gl_pX_yJXm"}},{"cell_type":"code","source":"train_data_loader, test_data_loader, val_data_loader = get_dataloader(\n    train_file_path, test_file_path, val_file_path,\n    tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T02:30:54.870038Z","iopub.status.idle":"2023-10-11T02:30:54.870645Z","shell.execute_reply.started":"2023-10-11T02:30:54.870424Z","shell.execute_reply":"2023-10-11T02:30:54.870444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = nn.DataParallel(ParaphraseIdentifier(PRETRAINED_MODEL_NAME)).to(device)\nloaded_model.load_state_dict(torch.load(MODEL_PATH))\ntest_acc, _ = eval_model(loaded_model, test_data_loader, loss_fn, device, len(test_df))\nprint(f'Test Accuracy: {test_acc}')","metadata":{"id":"0hahDAtmVJuI","execution":{"iopub.status.busy":"2023-10-11T02:30:54.874794Z","iopub.status.idle":"2023-10-11T02:30:54.875526Z","shell.execute_reply.started":"2023-10-11T02:30:54.875221Z","shell.execute_reply":"2023-10-11T02:30:54.875243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting predictions","metadata":{"id":"gSqvb5tAyRDA"}},{"cell_type":"code","source":"def get_predictions(model, data_loader, device):\n    model = model.eval()\n\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            labels = d[\"labels\"].to(device).tolist()\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            preds = outputs.argmax(-1).tolist()\n\n            all_labels.extend(labels)\n            all_preds.extend(preds)\n\n    return all_labels, all_preds","metadata":{"id":"uVuH026-VWgQ","execution":{"iopub.status.busy":"2023-10-11T02:30:54.876741Z","iopub.status.idle":"2023-10-11T02:30:54.877331Z","shell.execute_reply.started":"2023-10-11T02:30:54.877136Z","shell.execute_reply":"2023-10-11T02:30:54.877156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Model Performance","metadata":{"id":"DPxQ-PIMyVMH"}},{"cell_type":"code","source":"labels, preds = get_predictions(loaded_model, val_data_loader, device)\nscore  = accuracy_score(labels, preds)\nreport = classification_report(labels, preds, digits=4, output_dict=True)\nprint(\"Validation\")\nprint(score)\nprint(report)\npred_df = pd.DataFrame(report)\npred_df.to_csv(f'{training_dataset}_Validation.csv')","metadata":{"id":"elQc9XBrdhuB","outputId":"0596e35b-2ce6-4137-a723-86a1cb1b1297","execution":{"iopub.status.busy":"2023-10-11T02:30:54.887194Z","iopub.status.idle":"2023-10-11T02:30:54.887942Z","shell.execute_reply.started":"2023-10-11T02:30:54.887679Z","shell.execute_reply":"2023-10-11T02:30:54.887704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels, preds = get_predictions(loaded_model, test_data_loader, device)\nprint(\"BnPC\")\nscore  = accuracy_score(labels, preds)\nreport = classification_report(labels, preds, digits=4, output_dict=True)\nprint(score)\nprint(report)\npred_df = pd.DataFrame(report)\npred_df.to_csv(f'{training_dataset}_Test_BnPC.csv')","metadata":{"id":"bGeqs7PPWqot","outputId":"db50cb74-61ae-4b81-f71e-1c33ee771429","execution":{"iopub.status.busy":"2023-10-11T02:30:54.889135Z","iopub.status.idle":"2023-10-11T02:30:54.889817Z","shell.execute_reply.started":"2023-10-11T02:30:54.889572Z","shell.execute_reply":"2023-10-11T02:30:54.889615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in testing_files:\n    test_df = pd.read_csv(f'{path_to_dataset}/{i}.csv')\n    test_df = hide_numbers(test_df)\n    test_data_loader = create_data_loader(\n            test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n    labels, preds = get_predictions(loaded_model, test_data_loader, device)\n    print(f'{i}')\n    score  = accuracy_score(labels, preds)\n    report = classification_report(labels, preds, digits=4, output_dict=True)\n    print(score)\n    print(report)\n    pred_df = pd.DataFrame(report)\n    pred_df.to_csv(f'{training_dataset}_{i}.csv')","metadata":{},"execution_count":null,"outputs":[]}]}