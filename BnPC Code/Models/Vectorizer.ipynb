{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BNPP: Vectorizer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsHVsFwX41U+EiVQ1bbAZV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanShaheb34/Bangla-Paraphrase-Identification/blob/master/models/Vectorizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3amBv-IEZlHk",
        "outputId": "f0caf2f0-019d-4343-a3ad-2109c432076a"
      },
      "source": [
        "# ! gdown --id 1_C260zgutx-BXmMUkDYfhJTqlDLCsHPI\n",
        "# ! unzip FinalDataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_C260zgutx-BXmMUkDYfhJTqlDLCsHPI\n",
            "To: /content/FinalDataset.zip\n",
            "\r  0% 0.00/178k [00:00<?, ?B/s]\r100% 178k/178k [00:00<00:00, 70.6MB/s]\n",
            "Archive:  FinalDataset.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n",
            "  inflating: val.csv                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqCwHHlyZsJy",
        "outputId": "5e6a76ce-390a-4889-bffc-be3613f255f3"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_-bAL-GZtnb"
      },
      "source": [
        "def hide_numbers(df, token=' [CC] '):\n",
        "  df.sentence1 = [re.sub(r\"[^ঀ-ৣ ]\", '', s).strip() for s in df.sentence1]\n",
        "  df.sentence2 = [re.sub(r\"[^ঀ-ৣ ]\", '', s).strip() for s in df.sentence2]\n",
        "  return df\n",
        "\n",
        "def load_data():\n",
        "    train_df = hide_numbers(pd.read_csv('train.csv'))\n",
        "    test_df = hide_numbers(pd.read_csv('test.csv'))\n",
        "    val_df = hide_numbers(pd.read_csv('val.csv'))\n",
        "\n",
        "    return train_df, test_df, val_df\n",
        "\n",
        "def process_dataset(df, vectorizer):\n",
        "  x, y = [], []\n",
        "\n",
        "  for sent1, sent2, label in zip(df.sentence1, df.sentence2, df.label):\n",
        "    x.append(vectorizer.transform([sent1, sent2]).toarray().flatten())\n",
        "    y.append(label)\n",
        "\n",
        "  return x, y\n",
        "\n",
        "train_df, test_df, val_df = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSDvEhbLkMXv"
      },
      "source": [
        "ANALYZER = \"char\"\n",
        "TOKENIZER = nltk.word_tokenize\n",
        "RANGE = (2, 3)\n",
        "MAX_FEATURES = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH6Hk70xZ6tA",
        "outputId": "8e1a239e-0a9c-494c-c23c-730b5db21d37"
      },
      "source": [
        "sentences = train_df.sentence1 + train_df.sentence2\n",
        "vectorizer = TfidfVectorizer(analyzer=ANALYZER, tokenizer=TOKENIZER, ngram_range=RANGE, max_features=MAX_FEATURES).fit(sentences)\n",
        "\n",
        "train_x, train_y = process_dataset(train_df, vectorizer)\n",
        "test_x, test_y = process_dataset(test_df, vectorizer)\n",
        "val_x, val_y = process_dataset(val_df, vectorizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:552: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
            "  \"The parameter 'tokenizer' will not be used\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84sx2NgmcL_t",
        "outputId": "b07aafb7-d26b-44cf-be75-c1924f9939fb"
      },
      "source": [
        "model = SVC()\n",
        "model.fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zBPbWjEh_iC"
      },
      "source": [
        "def report(model, x, y):\n",
        "  preds = model.predict(x)\n",
        "  print(classification_report(y, preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ses4O9igjJTu",
        "outputId": "cdd9caa8-ef8b-4bc5-b21a-92687a6472fb"
      },
      "source": [
        "report(model, test_x, test_y)\n",
        "report(model, val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.96      0.76       823\n",
            "           1       0.75      0.18      0.28       571\n",
            "\n",
            "    accuracy                           0.64      1394\n",
            "   macro avg       0.69      0.57      0.52      1394\n",
            "weighted avg       0.68      0.64      0.56      1394\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.95      0.76       872\n",
            "           1       0.72      0.19      0.30       602\n",
            "\n",
            "    accuracy                           0.64      1474\n",
            "   macro avg       0.68      0.57      0.53      1474\n",
            "weighted avg       0.67      0.64      0.57      1474\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5C2RrgRxN6w",
        "outputId": "73e11d3c-9127-46a6-ba1a-44a23681f9b8"
      },
      "source": [
        "# vectorizer = TfidfVectorizer(max_features=200).fit(sentences)\n",
        "# vectorizer.get_feature_names_out()\n",
        "# vectorizer.transform([\"আমার সোনার বাংলা\", \"আমি তোমায় ভালোবাসি\"]).reshape((1,-1)).shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['অধ', 'অন', 'অপ', 'অবস', 'অভ', 'অর', 'অস', 'আইপ', 'আক', 'আগ', 'আজ',\n",
              "       'আটক', 'আত', 'আদ', 'আব', 'আম', 'আর', 'আরও', 'আল', 'আস', 'আসছ',\n",
              "       'ইউন', 'ইক', 'ইজ', 'ইট', 'ইড', 'ইত', 'ইন', 'ইম', 'ইয', 'ইর', 'ইল',\n",
              "       'ইস', 'ইয়', 'উজ', 'উড', 'উত', 'উদ', 'উস', 'এক', 'একদ', 'এখন', 'এগ',\n",
              "       'এনপ', 'এব', 'এল', 'ওব', 'ওয', 'ওয়', 'কট', 'কত', 'কথ', 'কন', 'কব',\n",
              "       'কম', 'কমল', 'কর', 'করছ', 'করত', 'করব', 'করল', 'কল', 'কলক', 'খবর',\n",
              "       'গণন', 'গবন', 'গর', 'গল', 'ঘণ', 'ঘর', 'চক', 'চট', 'চন', 'চল', 'ছব',\n",
              "       'জন', 'জয', 'জর', 'জল', 'জয়', 'টক', 'টগ', 'টট', 'টন', 'টব', 'ঠক',\n",
              "       'তক', 'তন', 'তব', 'তম', 'তর', 'তল', 'থগ', 'দক', 'দগ', 'দন', 'দর',\n",
              "       'দল', 'ধর', 'নক', 'নগ', 'নত', 'নন', 'নব', 'নম', 'নমন', 'নয', 'নস',\n",
              "       'নয়', 'পক', 'পড', 'পত', 'পথ', 'পদ', 'পন', 'পর', 'ফত', 'ফল', 'বক',\n",
              "       'বঙ', 'বছর', 'বজ', 'বড', 'বদন', 'বন', 'বব', 'বর', 'বরখ', 'বল',\n",
              "       'বলল', 'বস', 'বহ', 'ভর', 'মক', 'মত', 'মদ', 'মধ', 'মন', 'মপ', 'মর',\n",
              "       'মল', 'মস', 'মহ', 'যক', 'যকর', 'যম', 'রই', 'রও', 'রক', 'রজ', 'রণ',\n",
              "       'রত', 'রথম', 'রদ', 'রধ', 'রন', 'রপ', 'রফ', 'রব', 'রভ', 'রম', 'রয',\n",
              "       'রল', 'রশ', 'রস', 'রহ', 'রয়', 'লক', 'লজ', 'লড', 'লত', 'লদ', 'লন',\n",
              "       'লব', 'লম', 'লল', 'শক', 'শত', 'শন', 'শর', 'ষক', 'ষণ', 'সক', 'সঙ',\n",
              "       'সঞ', 'সদ', 'সন', 'সপ', 'সব', 'সম', 'সর', 'সরক', 'সহ', 'হচ', 'হত',\n",
              "       'হব', 'হয', 'হর', 'হল', 'হয়'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp-IqLPjxbRm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}